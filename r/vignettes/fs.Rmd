---
title: "Working with Cloud Storage (S3)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Working with Cloud Storage (S3)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The Arrow C++ library includes a generic filesystem interface and specific
implementations for some cloud storage systems. This setup allows various
parts of the project to be able to read and write data with different storage
backends. In the `arrow` R package, support has been enabled for AWS S3 on
macOS and Windows. This vignette provides an overview of working with S3 data
using Arrow.

> Note that S3 support is not enabled by default on Linux due to packaging complications. To enable it, you will need to build and install [aws-sdk-cpp](https://aws.amazon.com/sdk-for-cpp/) from source, then set the environment variable `EXTRA_CMAKE_FLAGS="-DARROW_S3=ON -DAWSSDK_SOURCE=SYSTEM"` prior to building the R package (with bundled C++ build, not with Arrow system libraries) from source.

## URIs

File readers and writers (`read_parquet()`, `write_feather()`, et al.)
now accept an S3 URI as the source or destination file,
as do `open_dataset()` and `write_dataset()`.
An S3 URI looks like:

```
s3://[id:secret@]bucket/path[?region=]
```

For example, one of the NYC taxi data files used in `vignette("dataset", package = "arrow")` is found at

```
s3://ursa-labs-taxi-data/2019/06/data.parquet?region=us-east-2
```

`region` defaults to `us-east-1` and can be omitted if the bucket is in that region.

Given this URI, we can pass it to `read_parquet()` just as if it were a local file path:

```r
df <- read_parquet("s3://ursa-labs-taxi-data/2019/06/data.parquet?region=us-east-2")
```

Note that this will be slower to read than if the file were local,
though if you're running on a machine in the same AWS region as the file in S3,
the cost of reading the data over the network should be much lower.

## Authentication

To access private S3 buckets, you need two secret parameters:
a `AWS_ACCESS_KEY_ID`, which is like a user id,
and `AWS_SECRET_ACCESS_KEY`, like a token.
There are a few options for passing these credentials:

1. Include them in the URI, like `s3://AWS_ACCESS_KEY_ID:AWS_SECRET_ACCESS_KEY@bucket-name/path/to/file`. Be sure to [URL-encode](https://en.wikipedia.org/wiki/Percent-encoding) your secrets if they contain special characters like "/".

2. Set them as environment variables named `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.

3. Define them in a `~/.aws/credentials` file, according to the [AWS documentation](https://docs.aws.amazon.com/sdk-for-cpp/v1/developer-guide/credentials.html).
